# üî¥ Redis Setup Guide for Heavy Traffic

**Why Redis?** Your current in-memory rate limiting and caching won't work with multiple server instances. Redis provides distributed storage that all instances can share.

**Time Required:** 2-3 hours  
**Priority:** HIGH (before heavy traffic)

---

## Option 1: Railway (Recommended - Easiest)

### Step 1: Create Redis Instance

1. Go to https://railway.app
2. Click "New Project"
3. Click "Add Service" ‚Üí "Database" ‚Üí "Redis"
4. Copy the connection URL (looks like: `redis://default:password@host:port`)

### Step 2: Add to Environment Variables

```bash
# Add to .env (NOT committed)
REDIS_URL=redis://default:password@host:port

# Add to production (Railway)
railway variables set REDIS_URL=redis://default:password@host:port
```

### Step 3: Install Redis Client

```bash
npm install ioredis
```

### Step 4: Create Redis Client

Create `src/lib/redis.ts`:

```typescript
import Redis from "ioredis";

let redis: Redis | null = null;

export function getRedisClient(): Redis {
  if (!redis) {
    redis = new Redis(process.env.REDIS_URL || "redis://localhost:6379", {
      maxRetriesPerRequest: 3,
      retryStrategy(times) {
        const delay = Math.min(times * 50, 2000);
        return delay;
      },
      reconnectOnError(err) {
        const targetError = "READONLY";
        if (err.message.includes(targetError)) {
          return true;
        }
        return false;
      },
    });

    redis.on("error", (err) => {
      console.error("Redis error:", err);
    });

    redis.on("connect", () => {
      console.log("Redis connected");
    });
  }

  return redis;
}

export default getRedisClient;
```

### Step 5: Update Rate Limiting

Create `src/lib/redis-rate-limit.ts`:

```typescript
import { getRedisClient } from "./redis";
import { NextRequest } from "next/server";
import { getClientIP } from "./security";

interface RateLimitResult {
  allowed: boolean;
  remaining: number;
  resetTime: number;
  riskScore: number;
  threatLevel: "low" | "medium" | "high" | "critical";
}

export async function checkRedisRateLimit(
  request: NextRequest,
  endpoint: string
): Promise<RateLimitResult> {
  const redis = getRedisClient();
  const clientIP = getClientIP(request);

  // Rate limit configuration
  const limits: Record<string, { maxRequests: number; windowMs: number }> = {
    "/api/send-career": { maxRequests: 2, windowMs: 15 * 60 * 1000 },
    "/api/send-contact": { maxRequests: 3, windowMs: 15 * 60 * 1000 },
    "/api/send-devis": { maxRequests: 3, windowMs: 15 * 60 * 1000 },
    "/api/blogs": { maxRequests: 60, windowMs: 60 * 1000 },
    "/api/blog-categories": { maxRequests: 30, windowMs: 60 * 1000 },
    "/api/media/[...path]": { maxRequests: 100, windowMs: 60 * 1000 },
    global: { maxRequests: 200, windowMs: 5 * 60 * 1000 },
  };

  const limit = limits[endpoint] || limits.global;
  const key = `rate_limit:${endpoint}:${clientIP}`;
  const windowSeconds = Math.ceil(limit.windowMs / 1000);

  try {
    // Use Redis INCR with expiry
    const current = await redis.incr(key);

    if (current === 1) {
      // First request in window, set expiry
      await redis.expire(key, windowSeconds);
    }

    const ttl = await redis.ttl(key);
    const resetTime = Date.now() + ttl * 1000;
    const remaining = Math.max(0, limit.maxRequests - current);

    // Calculate risk score
    const riskScore = Math.min(100, (current / limit.maxRequests) * 100);
    let threatLevel: "low" | "medium" | "high" | "critical" = "low";

    if (riskScore > 90) threatLevel = "critical";
    else if (riskScore > 75) threatLevel = "high";
    else if (riskScore > 50) threatLevel = "medium";

    return {
      allowed: current <= limit.maxRequests,
      remaining,
      resetTime,
      riskScore,
      threatLevel,
    };
  } catch (error) {
    console.error("Redis rate limit error:", error);
    // Fallback to allowing request if Redis fails
    return {
      allowed: true,
      remaining: 999,
      resetTime: Date.now() + limit.windowMs,
      riskScore: 0,
      threatLevel: "low",
    };
  }
}
```

### Step 6: Update Middleware

Edit `src/middleware/security.ts`:

Replace the rate limiting section with:

```typescript
import { checkRedisRateLimit } from "@/lib/redis-rate-limit";

// Inside withSecurity function:
const rateLimitResult = await checkRedisRateLimit(request, endpoint);
if (!rateLimitResult.allowed) {
  // ... existing rate limit exceeded logic
}
```

### Step 7: Add Caching

Create `src/lib/redis-cache.ts`:

```typescript
import { getRedisClient } from "./redis";

export async function getCachedData<T>(
  key: string,
  fetcher: () => Promise<T>,
  ttl: number = 3600
): Promise<T> {
  const redis = getRedisClient();

  try {
    // Try to get from cache
    const cached = await redis.get(key);
    if (cached) {
      return JSON.parse(cached) as T;
    }

    // Fetch fresh data
    const data = await fetcher();

    // Store in cache
    await redis.setex(key, ttl, JSON.stringify(data));

    return data;
  } catch (error) {
    console.error("Redis cache error:", error);
    // Fallback to fetcher if Redis fails
    return fetcher();
  }
}

export async function invalidateCache(pattern: string): Promise<void> {
  const redis = getRedisClient();

  try {
    const keys = await redis.keys(pattern);
    if (keys.length > 0) {
      await redis.del(...keys);
    }
  } catch (error) {
    console.error("Redis invalidate error:", error);
  }
}
```

### Step 8: Update Blog Service

Edit `src/lib/services/blogService.ts`:

Add at the top:

```typescript
import { getCachedData } from "../redis-cache";
```

Update `getBlogs` function:

```typescript
export const getBlogs = async (params) => {
  const cacheKey = `blogs:${JSON.stringify(params)}`;

  return getCachedData(
    cacheKey,
    async () => {
      // Existing fetch logic here
      const response = await fetch(/* ... */);
      return response.json();
    },
    900 // 15 minutes
  );
};
```

### Step 9: Test Locally

```bash
# Start local Redis (if testing locally)
docker run -d -p 6379:6379 redis:7-alpine

# Or use Railway Redis URL
export REDIS_URL=redis://default:password@host:port

# Build and test
npm run build
npm start

# Test rate limiting
for i in {1..70}; do
  curl http://localhost:3000/api/blogs
done
# Should see rate limiting after 60 requests
```

### Step 10: Deploy

```bash
# Deploy to Railway
railway up

# Or Vercel (with external Redis)
vercel --prod
```

---

## Option 2: Upstash (Serverless Redis)

### Step 1: Create Upstash Account

1. Go to https://upstash.com
2. Create account (free tier available)
3. Create new Redis database
4. Copy REST URL and token

### Step 2: Install Upstash Client

```bash
npm install @upstash/redis
```

### Step 3: Create Upstash Client

Create `src/lib/upstash.ts`:

```typescript
import { Redis } from "@upstash/redis";

export const redis = new Redis({
  url: process.env.UPSTASH_REDIS_REST_URL!,
  token: process.env.UPSTASH_REDIS_REST_TOKEN!,
});
```

### Step 4: Update Rate Limiting

Use same logic as Option 1, but import from `@upstash/redis` instead of `ioredis`.

---

## Option 3: Local Redis (Development Only)

### Step 1: Install Redis

**Windows:**

```bash
# Using Chocolatey
choco install redis-64

# Or download from: https://github.com/microsoftarchive/redis/releases
```

**Mac:**

```bash
brew install redis
brew services start redis
```

**Linux:**

```bash
sudo apt-get install redis-server
sudo systemctl start redis
```

### Step 2: Test Connection

```bash
redis-cli ping
# Should return: PONG
```

### Step 3: Use localhost URL

```bash
# .env
REDIS_URL=redis://localhost:6379
```

---

## Verification

### Test Rate Limiting

```bash
# Test blog endpoint
for i in {1..70}; do
  curl -s -o /dev/null -w "%{http_code}\n" https://trtbroker.com/api/blogs
done
# Should see 429 after 60 requests
```

### Test Caching

```bash
# First request (slow - fetches from Strapi)
time curl https://trtbroker.com/api/blogs

# Second request (fast - from Redis cache)
time curl https://trtbroker.com/api/blogs
```

### Check Redis Data

```bash
# Connect to Redis
redis-cli -u $REDIS_URL

# List all keys
KEYS *

# Check rate limit key
GET rate_limit:/api/blogs:127.0.0.1

# Check cache key
GET blogs:*
```

---

## Monitoring

### Add Redis Health Check

Edit `src/app/api/health/route.ts`:

```typescript
import { getRedisClient } from "@/lib/redis";

export async function GET() {
  try {
    const redis = getRedisClient();
    await redis.ping();

    return NextResponse.json({
      status: "healthy",
      services: {
        redis: "up",
        strapi: "up",
      },
    });
  } catch (error) {
    return NextResponse.json(
      {
        status: "unhealthy",
        services: {
          redis: "down",
          error: error.message,
        },
      },
      { status: 503 }
    );
  }
}
```

---

## Troubleshooting

### Connection Refused

```bash
# Check Redis is running
redis-cli ping

# Check URL is correct
echo $REDIS_URL

# Check firewall allows connection
telnet redis-host 6379
```

### High Memory Usage

```bash
# Check memory usage
redis-cli INFO memory

# Set max memory limit
redis-cli CONFIG SET maxmemory 256mb
redis-cli CONFIG SET maxmemory-policy allkeys-lru
```

### Slow Performance

```bash
# Check slow queries
redis-cli SLOWLOG GET 10

# Monitor commands
redis-cli MONITOR
```

---

## Cost Estimates

**Railway Redis:**

- Free tier: 512MB RAM
- Paid: $5-10/month for 1GB

**Upstash:**

- Free tier: 10,000 commands/day
- Paid: $0.20 per 100K commands

**Self-hosted:**

- VPS: $5-10/month
- Maintenance time: 2-4 hours/month

---

## Next Steps

After Redis is set up:

1. ‚úÖ Rate limiting is distributed
2. ‚úÖ Caching is shared across instances
3. ‚úÖ Can scale horizontally
4. ‚è≠Ô∏è Set up CDN (see CDN_SETUP_GUIDE.md)
5. ‚è≠Ô∏è Add monitoring (see MONITORING_SETUP_GUIDE.md)

---

**Status After Redis:** üü¢ READY FOR HEAVY TRAFFIC  
**Scalability:** ‚úÖ Can handle 1000+ concurrent users  
**Performance:** ‚úÖ 90% reduction in Strapi load
